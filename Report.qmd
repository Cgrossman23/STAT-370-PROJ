---
title: "STAT 370 - Wetland Plant Project"
format: pdf
author: Ashley Ciss, Coleton Grossman, William Quinn
editor: visual
indent: true
---

# I. Introduction

Along the St. Mary’s River in Michigan, there are shores being affected by the wake of freighter ships passing through. These wakes can be between one and three meters large and eventually stir up the areas of the river and cause plant growth disruption. In order to understand the effect of these waves, the density of the plants, bulrush and cattail need to be found. In this specific study, there were four different collection sites, each with a control side and an exposed side. Each site had key environmental variables measured. Some of the important ones include exposure, water depth, slope, fetch, and cost distance. The main goal of the data analysis was to create a predictive model that would output the density or presence/absence of these plants in this river.

Due to the spatial nature of the sites, there was spatial correlation and cluster correlation. Not only was each sight correlated with one another, there were transects vertical and horizontal that also contributed to the correlation. The spatial element of these sites play a significant role in understanding and creating an explanatory model. Understanding what the river looks like as well as the sites was one of the most important parts of the process. The data also did not follow a standard discrete distribution, so there were multiple tests completed to find the best fitting distribution for the data, which was a negative binomial distribution. Using both the correlation restrictions and the client’s needs, multiple models were created and tested against each other in order to assess fit. There were different types of models that can be created to accurately represent the data, such as mixed effects, zero inflated models, and logistic regression. The best model for the data that accurately predict the outcome of presence or absence was the logistic regression. Once the best model was found, there was a Bayesian analysis completed because the spatial correlation aspect does not need to be included in the model.

# II. Methods

The consulting process was divided into two major phases: an exploratory data analysis phase and explanatory modeling phase. Most of the statistical analysis and modeling was conducted via R-Studio while some handling of geospatial data was conducted via GIS.

### II.a. Data Visualization

The exploratory data analysis phase primarily constituted data visualizations such as bar graphs, scatter plots, and spatial data visualization maps. Key packages utilized in this phase were ggplot2, ggmap, and the sf package.

### II.b. Explanatory Modeling

The modeling phase of this project consisted of four distinct sub-phases: creating geographical correlation matrices, creating and testing zero-inflated models (ZIM’s), creating and testing mixed effect models (with and without ZIM elements), and the creation of two global models (a ZIM and a logistic regression model) with spatial lag explanatory variables. All models are explanatory models and seek to isolate and identify the influence of the variable *exposure* while controlling for the influence of potentially other significant variables such as *water_depth* and *speed_limit*.

#### II.b.1. **Geographical Correlation Matrices**

This code section was conducted with the purpose of fully capturing the maximum number of geographical correlations as defined under the Moran’s I equation. The Moran’s I equation resembles the standard variance equation; except that it seeks to account for the strength (or weakness) of geographical correlation via a weighted matrix. The formula for Moran’s I is as follows:

$$
I(d) = \frac{n \sum_{i=1}^{n}\sum_{j=1}^{n}w_{ij}(X_i - \bar{X})(X_j - \bar{X})}{(\sum_{i=1}^{n}\sum_{j=1}^{n}w_{ij}) \sum_{i=1}^{n}(X_i - \bar{X})^2}
$$

Moran’s I is a measurement that ranges from \[-1,1\]. A value of 1 represents perfect positive autocorrelation. This would be a geographical data set in which values are perfectly geographically close (or attracted) to other similarly valued observations. A value of -1 represents perfect negative autocorrelation and means that data values are perfectly geographical distant (or repulsed) by other similarly valued observations. Moran’s I values that are near 0 indicate a lack of geographical correlation.

It was found necessary to calculate this measure due to the perceived violation of geographical independence by our data set. Our preliminary data visualizations displayed that plots with plant presence tended to clump together in chunks, indicating the presence of positive autocorrelation. By calculating the Moran’s I measurement we were able to verify the validity or invalidity of non-geographic models so as to confirm the presence of geographical correlations. Furthermore, the spatial weights matrix produced was used to create the spatial lag elements in our final global model.

A separate spatial weights matrix was constructed for each of our 4 non-exposed plots due to the differing plant densities observed across them. No spatial weights matrix was constructed for the 4 exposed plots due to the lack of any plant present plots in these segments.  These spatial weights matrices account for are weighted in the sense that instead of accounting for presence/absence of roots, they account for the root density when identifying the strength between two neighboring observations. The pseudo-code for the creation of each of these 4 individual weights matrices and Moran’s I measurements is below. Via this function we produced Moran’s I measurements that ranged from roughly \[0.292, 0.81\]. This indicates a moderate to strong geographical influence on plant root density.

Pseudo-code:

// Step 1: Extract spatial coordinates

Set coordinates_matrix to the combination of latitude and longitude columns from the data set

// Step 2: Define a sequence of distance thresholds to test

Set distance_values to a sequence from the minimum to maximum distance (e.g., 0.000001 to 0.001 with small increments)

// Step 3: Initialize a container for Moran’s I values at each distance

Create moran_I_values as a list of NA values, same length as distance_values

// Step 4: Initialize variables to track the maximum Moran’s I and its associated distance

Set max_moran_I to negative infinity

Set optimal_distance to NA

Set optimal_neighbors to NULL

Set optimal_weights to NULL

// Step 5: For each candidate distance threshold, do:

For each distance in distance_values:

    - Identify neighbor relationships among coordinates using a fixed maximum distance

      neighbors ← all pairs of points within this distance

    - If no neighbors are found, skip to the next distance

    - Compute the distances between all neighbors

    - Construct spatial weights: inverse of distance, row-standardized

    - Calculate Moran’s I statistic for the outcome variable (e.g., population density) using the weights

    - Store the Moran’s I value

    - If this Moran’s I is the highest so far:

        • Update max_moran_I

        • Save the current distance as optimal_distance

        • Save the neighbor structure and weights

End pseudo code.

#### II.b.2. ZIM

The first explanatory models were constructed with just a zero-inflated element. These models were created in order to test for the significance of geographical correlations within each plot against a properly fitted model. So as to account for clustering effects 4 separate models were constructed: one for each unexposed plot per site. Once a properly fitted model was constructed for each plot, the residuals of the model were tested against the spatial weighted matrix of the plot. This was conducted via the moran.test command found in the *spdep* package.

The assumed underlying distribution for the zero-inflated model was a negative-binomial distribution. The formula for this model is as follows:

$$
P(y_i = 0) = \pi_i + (1 - \pi_i) * g(y_i=0)
$$

$$
P(y_i = k) = (1 - \pi_i)*g(y_i)
$$

Where $g(y_i)$ is the negative binomial distribution given by:

$$
g(y_i) = \Pr(Y = y_i \mid \mu_i, \alpha) = 
\frac{\Gamma(y_i + \alpha^{-1})}{\Gamma(\alpha^{-1}) \Gamma(y_i + 1)}
\left( \frac{1}{1 + \alpha \mu_i} \right)^{\alpha^{-1}}
\left( \frac{\alpha \mu_i}{1 + \alpha \mu_i} \right)^{y_i}
$$

And the logistic link function $\pi_i$ is given by:

$$
\pi_i=\dfrac{\lambda_i}{1+\lambda_i}
$$

#### II.b.3. Mixed-Effect Models

Due to the cluster aspect of the data and four different collection sites, a mixed effect model was employed so as to account for cluster correlation. The clustered nature of the locations causes a lack of independence, which indicates that a random effect would help represent these clusters. Using the lme4 package in R, a mixed effect model was created. While both random intercepts and slopes were tested, the model only converged when allowing for random intercepts across clusters.

#### **II.b.4. Global Spatial-Lag Models**

Lastly, the creation of a global model was conducted in order to create a summary measure of the influence of the exposure variable across sites. This involved the creation of two models: a zero-inflated model which accounts for root density, and a logistic regression model that simply accounts for plant presence or absence.

#### **II.b.4.1. Global ZIM**

This model combines elements of a zero-inflated model, random intercepts, and geographical autocorrelation, where spatial lags are entered as predictor variables for each individual observation.  The underlying distribution is as follows:

$$
\text{logit}(\pi_i) = \gamma_0
$$

$$
P(Y_i = y) = \begin{cases} \pi_i + (1-\pi_i) \cdot f(0; \mu_i, \theta), &  y = 0 \\ (1- \pi_i) \cdot f(y;\mu_i,\theta), &  y > 0 \end{cases}
$$

Furthermore, the negative binomial mean is calculated using the coefficient estimates for our explanatory variables and our spatial lag variables.

$$
Y_i \sim \text{NegBin}(\mu_i, \theta)
$$

Lastly, our random intercept effects follow a normal distribution centered at a mean of 0 and a standard deviation derived from variance observed across *site_individual*.

#### **II.b.4.2. Global Logistic Regression**

The global logistic regression model constructed instead removes the zero-inflated aspect of the prior model. This model loses the nuance of root density counts in favor of simplicity and ease of interpretation. The underlying distribution is assumed to be a Bernoulli process with the following formation:

Again here our random intercepts follow a normal distribution centered at 0 and a standard deviation that is derived from variances across *site_individual*.

# III. Results

### III.a. Preliminary Mixed-Effects Models

It was clear from the exploratory data analysis as well as the calculation of Moran’s I values that there was geographical correlation within the sites. However, the first step to try and account for correlation was a mixed effect model. These models did not account for individual observation spatial correlation structures and were conducted with the intent of measuring the necessity and importance of random intercepts for clusters. These models indicated significance of our explanatory variables, however, there is still some issues with correlation, as well as large numbers of zero response observations.

The first model ran was a baseline model with the only explanatory variable being random intercepts for individual sites. Using this model, the calculated r-squared was found to be 15.85%, so about 15.85% of the variance was due to the clustered nature of the sites. This model had an AIC value of roughly 2377.

The second model added exposure as an explanatory variable while retaining random intercepts. This model had a marginal r-squared of 11.46% and a conditional r-squared value of 15.18%. This conditional r-squared represents the variance accounted from the inclusions of both the random intercepts and the explanatory variables; while the marginal r-squared indicates only the variance accounted for by the explanatory variable exposure. Comparing the baseline model with model with exposure, using a log-likelihood test, this second model was statistically significantly better at explaining variance than the baseline model, indicating that exposure is a significant variable. This was also consistent with the AIC and BIC values found.

A third model was constructed including water depth as another explanatory variable. The marginal r-squared was found to be about 12.85%, while the conditional r-squared value was found to be 17.15%. Running the same likelihood tests, this full model was better; indicating that accounting for water depth may be important. A model with an interaction term which was tested and slightly better, but the difference was minimal.

| Model                                                                         | R-Squared Marginal | R-Squared Conditional | AIC    | Intercept Variance |
|---------------|---------------|---------------|---------------|---------------|
| Baseline with random intercepts                                               | 0                  | 0.1584586             | 2377   | 7.807              |
| Exposure with random intercepts                                               | 0.1146265          | 0.1517742             | 2370.4 | 1.816              |
| Water depth, exposure, and random intercept                                   | 0.1285217          | 0.1716464             | 2366.8 | 2.126              |
| Water depth, exposure, interaction term, and individual site random intercept | 0.1399794          | 0.1928591             | 2364.8 | 2.643              |

There were four different mixed effects model created each with a random intercept and different levels of predictor variables. The best model with a random intercept was the full model, where the interaction term had a negligible significance. The differences in AIC can be seen in Table 1. Lastly, as table 1 shows, random intercept variance is decreased when explanatory variables are included in comparison to the baseline model, suggesting that the significance of cluster correlation decreases as variables are included.

### III.b. Preliminary Zero Inflated Models

Due to the large number of zeros that were included in the data, a zero inflated model appeared to be a method that would yield significant results. However, even though these analyses deemed appropriate due to the clustering of the four individual sites and the zero-inflated aspect, there is still correlation horizontally and vertically that needs to be accounted for in the model. These models were constructed in order to test the validity of a zero-inflated approach.

Four models were fit in this phase:

-   A baseline model with just random intercepts

-   A full model in which exposure, water depth, and slope were included in both the structural and conditional formulas

-   A reduced model in which water depth and slope were included in the conditional formual and exposure was only included in the structural formula.

-   A further reduced model which did not have random intercepts

| Model                                 | AIC    | Intercept Variance |
|---------------------------------------|--------|--------------------|
| Baseline model with random intercepts | 807.79 | 15.27              |
| Full model with random intercepts     | 712.90 | 1.722e-09          |
| Reduced model with random intercepts  | 707.68 | 1.373e-09          |
| Reduced model, singular intercept     | 705.68 | NA                 |

When incorporating the zero-inflated model aspect with the same predictors, the AIC was drastically decreased in comparison to the simply mixed-effect models, indicating that the zero inflated aspect was significant. Furthermore, the p-value of the zero-inflated aspect of these models was always statistically significant. The first model fitted was again a baseline model. This model had very large variance for the random intercepts.

The full model had significantly better fit than the baseline model, but the variance for the random intercepts dropped to a value that was essentially zero. This indicates that inclusion of our explanatory variables makes any differences across clusters negligible to the point of non-existence. However, theoretically and statistically it did not make sense for exposure to be included in the conditional formula and for water depth or slope to be included in the structural formula. This was validated by the full model which indicated that these variables were insignificant in these respective formulas. For reference:

-   The structural formula explains whether an observation is an inflated zero count or not

-   The conditional formula explains variance in root density when plants are present

Indeed, this was validated by the reduced model which had better fit measures than the full model. However, the lack of significance of the random intercepts remained due to the extremely small variance seen. The final model found that excluding these random intercepts was of statistically significantly better fit. Theoretically, this can be justified due to a potential lack of significant

### III.c. Global Spatial Models

The global spatial models are a less sophisticated approach to addressing spatial correlation between individual observations. In this model, the optimal distance used to consider observations as neighbors or not is held constant across sites. Thus, this model loses some ability for accounting for spatial correlation. The full model here uses water depth and slope in the conditional model and exposure in the structural model.

| Model                           | AIC   | Intercept Variance |
|---------------------------------|-------|--------------------|
| Baseline with random intercepts | 734.8 | 13.63              |
| Full with random intercepts     | 709.6 | 1.042e-09          |
| Full with singular intercept    | 707.6 | NA                 |

Again, our full model is of better fit than the baseline. This suggests that water depth and slope are statistically significant in predicting root density, while exposure is statistically significant when predicting presence or absence of plant life. Furthermore, we again see that our random intercept variance drops to a near zero level in our full model, indicating that random intercepts are unnecessary. This is confirmed by the final model which has a lower AIC score with no random effects.

### III.d. Site-Specific Spatial Models

The local spatial model employed here is a slightly more sophisticated approach in which the optimum distance for considering individual observations as neighbors is allowed to vary across sites, allowing for a more accurate accounting of spatial correlations. The full model here uses water depth and slope in the conditional model and exposure in the structural model.

| Model                           | AIC    | Intercept Variance |
|---------------------------------|--------|--------------------|
| Baseline with random intercepts | 763.12 | 14.71              |
| Full with random intercepts     | 708.1  | 8.191e-10          |
| Full with singular intercept    | 706.15 | NA                 |

Overall, previous trends hold true for these models. Our full model is of better fit than our baseline model and we again find that random intercepts become unnecessary when accounting for exposure, water depth, and slope. Lastly, we find that our full model here has a slightly lower AIC score than our global full model.

### **III.e. Site-Specific Logistic Regression Model**

The logistic regression that focuses on purely predicting the presence or absence of plant life is a significantly less complex model. This allows the inclusion of higher order spatial correlation terms which further reduces spatial correlation violatoins. Due to the removal of plant density measures, the significance of water depth and slope was lost, leading to exposure being the sole predictor in the full model. Notably, random intercepts remained statistically significant in this mode. The best model found was the full model with random intercepts included.

| Model                           | AIC    | Intercept Variance |
|---------------------------------|--------|--------------------|
| Baseline with random intercepts | 167.3  | 4.904              |
| Full with random intercepts     | 163.5  | 2.304              |
| Full with singular intercept    | 190.40 | NA                 |

### **III.f. Model Comparison**

| Model                             | AIC    | P-Value Associated with Exposure | Exposure Odds Ratio |
|-------------------|------------------|------------------|------------------|
| Global Zero Inflated              | 707.6  | 5.56e-06 \*\*\*                  | 101                 |
| Site-Specific Zero Inflated       | 706.15 | 5.58e-06 \*\*\*                  | 100.79              |
| Site-Specific Logistic Regression | 163.5  | 0.0171 \*                        | 49.43               |

The zero inflated model and the logistic regression have different characteristics that could be more useful for the data and accurately predict presence or absence. The two zero inflated models are complex and interpretation may be difficult. However, both models maintain the nuance provided by root density measures. The observed and predicted values of the site specific zero inflated can be seen graphed in Graph 1. Logistic regression provides a slightly simpler and interpretable model. It does lose some of the detail in the root density that the zero inflated model preserves. In Graph 2, it appears that the model accurately can predict the presence or absence binary response. Based on the observed values of AIC and odds ratios, the logistic regression with site specific spatial lag predictor variables is the best fit for the data.

```{r, warning=FALSE, message=FALSE}
#| include: false
library(RCurl)
library(janitor)
library(dplyr)
library(car)
library(flexplot)
library(MASS)
library(ggplot2)

#mixed model libraries
library(lme4)
library(HLMdiag)
library(flexplot)
library(MuMIn)
library(glmmTMB) 

#geographic regression libraries
library(spdep)
library(deldir)

#for zero inflated
library(pscl)
```

```{r, results='hide'}
#| include: false

data_link <- getURL("https://raw.githubusercontent.com/Cgrossman23/STAT-370-PROJ/refs/heads/main/cleaned_data.csv") 
wave <- read.csv(text = data_link)
wave <- as.data.frame(wave)


# make all var undercase

wave <- clean_names(wave)

# renaming response variables to better names

colnames(wave)[7] <- "presence_absence_sch"
colnames(wave)[8] <- "density_sch"
colnames(wave)[9] <- "presence_absence_typ"
colnames(wave)[10] <- "density_typ"
names(wave)

names(wave)

wave[, c(1,2,7:15)] <- lapply(wave[, c(1,2,7:15)], function(x) gsub("[^0-9.]", "", x))
wave[, c(1,2,7:15)] <- lapply(wave[, c(1,2,7:15)], as.numeric)

str(wave)

unique(wave$transect) 
unique(wave$site_combined) 
unique(wave$site_individual)
unique(wave$exposure)

rows_with_na <- wave[!complete.cases(wave), ] 
print(rows_with_na)  
unique(rows_with_na$site_combined)
wave <- na.omit(wave)

# Summarazing response variables
wave$presence_absence <- (wave$presence_absence_sch + wave$presence_absence_typ)
wave$density <- (wave$density_sch + wave$density_typ)

wave <- wave %>% mutate(plant_type = case_when( 
                presence_absence_typ ==  1 ~ "typ", 
                presence_absence_sch == 1 ~ "sch", 
                TRUE ~ NA_character_ ))


# Filter for Unexposed and Site

wave$longitude <- ifelse(wave$longitude > 0, -wave$longitude, wave$longitude)

wave2 <- wave %>% filter(exposure == "Unexposed")
wave3 <- wave %>% filter(exposure == "Exposed")

wave_a <- wave2 %>% filter(site_combined == "A")
wave_b <- wave2 %>% filter(site_combined == "B")
wave_c <- wave2 %>% filter(site_combined == "C")
wave_d <- wave2 %>% filter(site_combined == "D")

wave_a2 <- wave3 %>% filter(site_combined == "A")
wave_b2 <- wave3 %>% filter(site_combined == "B")
wave_c2 <- wave3 %>% filter(site_combined == "C")
wave_d2 <- wave3 %>% filter(site_combined == "D")

wave4 <- rbind(wave_a, wave_b, wave_c, wave_d, wave_a2, wave_b2, wave_c2, wave_d2)
```

```{r}
#| include: false
coords_a <- cbind(wave_a$latitude, wave_a$longitude)

distances <- seq(0.000001, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi_a <- -Inf  
od_a <- NA
neighbors_a <- NULL
weights_a <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords_a, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords_a) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave_a$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi_a) {
    mmi_a <- values[i]
    od_a <- distances[i]
    neighbors_a <- neighbors
    weights_a <- weights
  }
}
cat("Maximum Moran's I:", mmi_a, "\n")
cat("Optimal Distance:", od_a, "\n")

plot(neighbors_a,coords_a, col = "red")
```

```{r}
#| include: false
coords_b <- cbind(wave_b$latitude, wave_b$longitude)
  
distances <- seq(0.00012, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi_b <- -Inf  
od_b <- NA
neighbors_b <- NULL
weights_b <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords_b, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords_b) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave_b$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi_b) {
    mmi_b <- values[i]
    od_b <- distances[i]
    neighbors_b <- neighbors
    weights_b <- weights
  }
}
cat("Maximum Moran's I:", mmi_b, "\n")
cat("Optimal Distance:", od_b, "\n")

plot(neighbors_b,coords_b, col = "red")
```

```{r}
#| include: false

coords_c <- cbind(wave_c$latitude, wave_c$longitude)
  
distances <- seq(0.000001, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi_c <- -Inf  
od_c <- NA
neighbors_c <- NULL
weights_c <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords_c, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords_c) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave_c$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi_c) {
    mmi_c <- values[i]
    od_c <- distances[i]
    neighbors_c <- neighbors
    weights_c <- weights
  }
}
cat("Maximum Moran's I:", mmi_c, "\n")
cat("Optimal Distance:", od_c, "\n")

plot(neighbors_c,coords_c, col = "red")
```

```{r}
#| include: false

coords_d <- cbind(wave_d$latitude, wave_d$longitude)

distances <- seq(0.00012, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi_d <- -Inf  
od_d <- NA
neighbors_d <- NULL
weights_d <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords_d, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords_d) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave_d$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi_d) {
    mmi_d <- values[i]
    od_d <- distances[i]
    neighbors_d <- neighbors
    weights_d <- weights
  }
}
cat("Maximum Moran's I:", mmi_d, "\n")
cat("Optimal Distance:", od_d, "\n")

plot(neighbors_d,coords_d, col = "red")
```

```{r}
#| include: false

coords <- cbind(wave4$latitude, wave4$longitude)
distances <- seq(0.00012, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi <- -Inf  
od <- NA
neighbors <- NULL
weights <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave4$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi) {
    mmi <- values[i]
    od <- distances[i]
    neighbors <- neighbors
    weights <- weights
  }
}
cat("Maximum Moran's I:", mmi, "\n")
cat("Optimal Distance:", od, "\n")

plot(neighbors,coords, col = "red")

# Compute Moran’s I
moran_test <- moran.test(wave4$density, listw = weights, zero.policy = TRUE)
print(moran_test)

# Global Moran's I is highly significant.
```

```{r}
#| include: false

wave4$spatial_lag1 <- NA
wave4$spatial_lag2 <- NA
wave4$spatial_lag3 <- NA

weights_list <- list(
  "A" = weights_a,
  "D" = weights_b,
  "F" = weights_c,
  "H" = weights_d
)

for (s in unique(wave4$site_individual)) {
  if (s %in% names(weights_list)) {
    site_subset <- wave4[wave4$site_individual == s, ]
    W <- weights_list[[s]]
    wave4$spatial_lag1[wave4$site_individual == s] <- lag.listw(W, site_subset$density)
    wave4$spatial_lag2[wave4$site_individual == s] <- lag.listw(W, wave4$spatial_lag1[wave4$site_individual == s])
    wave4$spatial_lag3[wave4$site_individual == s] <- lag.listw(W, wave4$spatial_lag2[wave4$site_individual == s])
  } else {
    wave4$spatial_lag1[wave4$site_individual == s] <- 0
    wave4$spatial_lag2[wave4$site_individual == s] <- 0
    wave4$spatial_lag3[wave4$site_individual == s] <- 0
  }
}
```

```{r}
#| include: false

spatial1 <- glmmTMB(density ~ spatial_lag1 + 
                              (1 | site_individual),
                              ziformula = ~ 1,
                              family = nbinom2,
                              data = wave4)

summary(spatial1)

spatial2 <- glmmTMB(density ~ water_depth +
                              slope_use +
                              spatial_lag1 + 
                              (1 | site_individual),
                              ziformula = ~ exposure,
                              family = nbinom2,
                              data = wave4)
 
summary(spatial2)
 
anova(spatial2, spatial1)

spatial3 <- glmmTMB(density ~ water_depth +
                              slope_use +
                              spatial_lag1,
                              ziformula = ~ exposure,
                              family = nbinom2,
                              data = wave4)

summary(spatial3)

anova(spatial3, spatial2)

wave4$residuals_local <- residuals(spatial3)
 
for (s in c("A", "D", "F", "H")) {
   site_subset <- wave4[wave4$site_individual == s, ]
   moran_test <- moran.test(site_subset$residuals_local, weights_list[[s]])
   print(paste("Moran's I for site", s, ":", moran_test$estimate["Moran I statistic"]))
 }
```

```{r}
#| echo: false
wave4$predicted_count <- predict(spatial3, type = "response")

ggplot(wave4, aes(x = water_depth, y = density)) +
  geom_jitter(aes(color = exposure), width = 0.1, height = 0.1, alpha = 0.4) +
  geom_smooth(aes(y = predicted_count, color = exposure), method = "loess", se = FALSE, size = 1.2) +
  labs(
    x = "Water Depth",
    y = "Observed Count",
    title = "Observed vs. Predicted Counts by Water Depth and Exposure",
    color = "Exposure"
  ) +
  theme_minimal()
```

```{r}
#| include: false

 wave4$presence_absence <- (wave4$presence_absence_sch + wave4$presence_absence_typ)

spatial_l1 <- glmmTMB(presence_absence ~ spatial_lag1 + 
                                        spatial_lag2 + 
                                        spatial_lag3 + 
                                        (1 | site_individual), 
                                        data = wave4, 
                                        family = binomial(link = "logit")) 

summary(spatial_l1)

spatial_l2 <- glmmTMB(presence_absence ~ exposure + 
                                        spatial_lag1 + 
                                        spatial_lag2 + 
                                        spatial_lag3 + 
                                        (1 | site_individual), 
                                        data = wave4, 
                                        family = binomial(link = "logit"))
 
summary(spatial_l2)

anova(spatial_l2, spatial_l1)

spatial_l3 <- glmmTMB(presence_absence ~ exposure + 
                                        spatial_lag1 + 
                                        spatial_lag2 + 
                                        spatial_lag3, 
                                        data = wave4, 
                                        family = binomial(link = "logit"))
 
summary(spatial_l3)

anova(spatial_l3, spatial_l2)
 
wave4$residuals_logistic <- residuals(spatial_l2)
 
for (s in c("A", "D", "F", "H")) {
   site_subset <- wave4[wave4$site_individual == s, ]
   moran_test <- moran.test(site_subset$residuals_logistic, weights_list[[s]])
   print(paste("Moran's I for site", s, ":", moran_test$estimate["Moran I statistic"]))
}
```

```{r}
#| echo: false

wave4$predicted_prob <- predict(spatial_l2, type = "response")

ggplot(wave4, aes(x = predicted_prob, y = presence_absence, color = exposure)) +
  geom_jitter(height = 0.05, width = 0, alpha = 0.3) +
  geom_smooth(method = "loess", formula = y ~ x, se = FALSE, size = 1.2) +
  labs(
    x = "Predicted Probability",
    y = "Observed Presence (1) / Absence (0)",
    title = "Observed vs. Predicted Probabilities by Exposure",
    color = "Exposure"
  ) +
  theme_minimal()
```

# IV. Conclusion

The main goal of the project and the client was to find a suitable, accurate, and interpretable model for the St. Mary’s River. With understanding the distributions, creating spatial lags to account for correlation, and the inclusions of mixed effects, the best model was the site-specific logistic regression. The binary response variable was deemed to be the most appropriate, due to the large number of zeros.

Although a valid logistic regression was an output of continued data analysis, there could have been a more thorough analysis completed with the Bayesian analysis as well as understanding the site-specific correlation. In the future, given more time and more data, a less complex correlation structure may yield better results. The zero inflated data and small number of observations did not help the analysis process and given more data these issues could be remediated.

\newpage

# Appendix {.appendix}

## Code

#### I. Libraries

```{r, warning=FALSE, message=FALSE}
#| echo: false
library(RCurl)
library(janitor)
library(dplyr)
library(car)
library(flexplot)
library(MASS)
library(ggplot2)

#mixed model libraries
library(lme4)
library(HLMdiag)
library(flexplot)
library(MuMIn)
library(glmmTMB) 

#geographic regression libraries
library(spdep)
library(deldir)

#for zero inflated
library(pscl)
```

#### II. Data Manipulation and Cleaning

```{r, results='hide'}
data_link <- getURL("https://raw.githubusercontent.com/Cgrossman23/STAT-370-PROJ/refs/heads/main/cleaned_data.csv") 
wave <- read.csv(text = data_link)
wave <- as.data.frame(wave)


# make all var undercase

wave <- clean_names(wave)

# renaming response variables to better names

colnames(wave)[7] <- "presence_absence_sch"
colnames(wave)[8] <- "density_sch"
colnames(wave)[9] <- "presence_absence_typ"
colnames(wave)[10] <- "density_typ"
names(wave)

names(wave)

wave[, c(1,2,7:15)] <- lapply(wave[, c(1,2,7:15)], function(x) gsub("[^0-9.]", "", x))
wave[, c(1,2,7:15)] <- lapply(wave[, c(1,2,7:15)], as.numeric)

str(wave)

unique(wave$transect) 
unique(wave$site_combined) 
unique(wave$site_individual)
unique(wave$exposure)

rows_with_na <- wave[!complete.cases(wave), ] 
print(rows_with_na)  
unique(rows_with_na$site_combined)
wave <- na.omit(wave)

# Summarazing response variables
wave$presence_absence <- (wave$presence_absence_sch + wave$presence_absence_typ)
wave$density <- (wave$density_sch + wave$density_typ)

wave <- wave %>% mutate(plant_type = case_when( 
                presence_absence_typ ==  1 ~ "typ", 
                presence_absence_sch == 1 ~ "sch", 
                TRUE ~ NA_character_ ))


# Filter for Unexposed and Site

wave$longitude <- ifelse(wave$longitude > 0, -wave$longitude, wave$longitude)

wave2 <- wave %>% filter(exposure == "Unexposed")
wave3 <- wave %>% filter(exposure == "Exposed")

wave_a <- wave2 %>% filter(site_combined == "A")
wave_b <- wave2 %>% filter(site_combined == "B")
wave_c <- wave2 %>% filter(site_combined == "C")
wave_d <- wave2 %>% filter(site_combined == "D")

wave_a2 <- wave3 %>% filter(site_combined == "A")
wave_b2 <- wave3 %>% filter(site_combined == "B")
wave_c2 <- wave3 %>% filter(site_combined == "C")
wave_d2 <- wave3 %>% filter(site_combined == "D")

wave4 <- rbind(wave_a, wave_b, wave_c, wave_d, wave_a2, wave_b2, wave_c2, wave_d2)
```

#### III. Geographical Correlation

##### III.a. Site A

```{r}
# Site A
coords_a <- cbind(wave_a$latitude, wave_a$longitude)

distances <- seq(0.000001, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi_a <- -Inf  
od_a <- NA
neighbors_a <- NULL
weights_a <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords_a, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords_a) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave_a$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi_a) {
    mmi_a <- values[i]
    od_a <- distances[i]
    neighbors_a <- neighbors
    weights_a <- weights
  }
}
cat("Maximum Moran's I:", mmi_a, "\n")
cat("Optimal Distance:", od_a, "\n")

plot(neighbors_a,coords_a, col = "red")
```

##### III.b. Site B

```{r}
coords_b <- cbind(wave_b$latitude, wave_b$longitude)
  
distances <- seq(0.00012, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi_b <- -Inf  
od_b <- NA
neighbors_b <- NULL
weights_b <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords_b, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords_b) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave_b$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi_b) {
    mmi_b <- values[i]
    od_b <- distances[i]
    neighbors_b <- neighbors
    weights_b <- weights
  }
}
cat("Maximum Moran's I:", mmi_b, "\n")
cat("Optimal Distance:", od_b, "\n")

plot(neighbors_b,coords_b, col = "red")
```

##### III.c. Site C

```{r}
coords_c <- cbind(wave_c$latitude, wave_c$longitude)
  
distances <- seq(0.000001, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi_c <- -Inf  
od_c <- NA
neighbors_c <- NULL
weights_c <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords_c, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords_c) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave_c$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi_c) {
    mmi_c <- values[i]
    od_c <- distances[i]
    neighbors_c <- neighbors
    weights_c <- weights
  }
}
cat("Maximum Moran's I:", mmi_c, "\n")
cat("Optimal Distance:", od_c, "\n")

plot(neighbors_c,coords_c, col = "red")
```

##### III.d. Site D

```{r}
coords_d <- cbind(wave_d$latitude, wave_d$longitude)

distances <- seq(0.00012, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi_d <- -Inf  
od_d <- NA
neighbors_d <- NULL
weights_d <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords_d, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords_d) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave_d$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi_d) {
    mmi_d <- values[i]
    od_d <- distances[i]
    neighbors_d <- neighbors
    weights_d <- weights
  }
}
cat("Maximum Moran's I:", mmi_d, "\n")
cat("Optimal Distance:", od_d, "\n")

plot(neighbors_d,coords_d, col = "red")
```

#### IV. Zero Inflated Models

##### IV.a. Site A

```{r}
model_zero_a <- zeroinfl(density ~ water_depth , data = wave_a, dist="negbin")

summary(model_zero_a)

negbin_model_b <- glm.nb(density ~ water_depth, data = wave_a)
vuong(model_zero_a, negbin_model_b)

poisson_model_b <- glm(density ~ water_depth, family = poisson, data = wave_a)
vuong(model_zero_a, poisson_model_b)

AIC(model_zero_a, negbin_model_b)
BIC(model_zero_a, negbin_model_b)

observed_a <- table(wave_a$density)
predicted_a <- table(round(predict(model_zero_a, type = "response")))

barplot(rbind(observed_a, predicted_a), beside = TRUE, col = c("black", "red"), legend.text = c("Observed", "Predicted"))

model_zero_a$model_zero_a$df.residual

###

resid_a <- residuals(model_zero_a)

moran_resid_test <- moran.test(resid_a, weights_a)

print(moran_resid_test) # insignificant
```

##### IV.b. Site B

```{r}
model_zero_b <- zeroinfl(density ~ water_depth , data = wave_b, dist="negbin")
model_zero_b2 <- zeroinfl(density ~ water_depth , data = wave_b, dist="poisson")

summary(model_zero_b)
summary(model_zero_b2)

negbin_model_b <- glm.nb(density ~ water_depth, data = wave_b)
vuong(model_zero_b, negbin_model_b)

poisson_model_b <- glm(density ~ water_depth, family = poisson, data = wave_b)
vuong(model_zero_b2, poisson_model_b)

AIC(model_zero_b, poisson_model_b)
BIC(model_zero_b, poisson_model_b)

observed_b <- table(wave_b$density)
predicted_b <- table(round(predict(model_zero_b, type = "response")))

barplot(rbind(observed_b, predicted_b), beside = TRUE, col = c("black", "red"), legend.text = c("Observed", "Predicted"))

###

resid_b <- residuals(model_zero_b)

moran_resid_test <- moran.test(resid_b, weights_b)

print(moran_resid_test) # insignificant
```

##### IV.c. Site C

```{r}
model_zero_c <- zeroinfl(density ~ water_depth , data = wave_c, dist="negbin")
model_zero_c2 <- zeroinfl(density ~ water_depth , data = wave_c, dist="poisson")

summary(model_zero_c)
summary(model_zero_c2)

negbin_model_c <- glm.nb(density ~ water_depth, data = wave_c)
vuong(model_zero_c, negbin_model_c)

poisson_model_c <- glm(density ~ water_depth, family = poisson, data = wave_c)
vuong(model_zero_c2, poisson_model_c)


AIC(model_zero_c, negbin_model_c)
BIC(model_zero_c, negbin_model_c)

BIC(model_zero_c2, poisson_model_c)
AIC(model_zero_c2, poisson_model_c)


observed_c <- table(wave_c$density)
predicted_c <- table(round(predict(model_zero_c, type = "response")))

barplot(rbind(observed_c, predicted_c), beside = TRUE, col = c("black", "red"),
        legend.text = c("Observed", "Predicted"))

###

resid_c <- residuals(model_zero_c)

moran_resid_test <- moran.test(resid_c, weights_c)

print(moran_resid_test) # significant
```

##### IV.d. Site D

```{r}
model_zero_d <- zeroinfl(density ~ water_depth , data = wave_d, dist="negbin")
model_zero_d2 <- zeroinfl(density ~ water_depth , data = wave_d, dist="poisson")

summary(model_zero_d)
summary(model_zero_d2)

negbin_model_d <- glm.nb(density ~ water_depth, data = wave_d)
vuong(model_zero_d, negbin_model_d)

poisson_model_d <- glm(density ~ water_depth, family = poisson, data = wave_d)
vuong(model_zero_d2, poisson_model_d)

AIC(model_zero_d, negbin_model_d)
BIC(model_zero_d, negbin_model_d)

AIC(model_zero_d2, poisson_model_d)
BIC(model_zero_d2, poisson_model_d)

observed_d <- table(wave_d$density)
predicted_d <- table(round(predict(model_zero_d, type = "response")))

barplot(rbind(observed_d, predicted_d), beside = TRUE, col = c("black", "red"), legend.text = c("Observed", "Predicted"))

###

resid_d <- residuals(model_zero_d)

moran_resid_test <- moran.test(resid_d, weights_d)

print(moran_resid_test) # significant
```

#### V. Mixed Effects Models

##### V.a. Baseline

```{r}
model1 <- lmer(density ~ 1 + (1 | site_individual), data = wave4)
summary(model1)
r_squared <- r.squaredGLMM(model1)
print(r_squared)
AIC(model1)
```

##### V.b. Exp - Exposure Nested in site_individual

```{r}
model2 <- lmer(density ~ exposure + (1 | site_individual), data = wave4)
r_squared2 <- r.squaredGLMM(model2)
print(r_squared2)
anova(model2, model1)
summary(model2)
```

##### V.c. Exp - Full Explanatory Variables

```{r}
model3 <- lmer(density ~  water_depth + exposure + (1 | site_individual), data = wave4)
r_squared3 <- r.squaredGLMM(model3)
print(r_squared3)
anova(model3, model2)
summary(model3)
```

##### V.d. Exp - Interaction term with Exposure nested in site_combined

```{r}
model4 <- lmer(density ~ exposure*water_depth + (1 | site_individual), data = wave4)
summary(model4)
r_squared4 <- r.squaredGLMM(model4)
print(r_squared4)
anova(model4, model3)
```

#### VI. ZIM with Mixed Effects

```{r}
model01 <- glmmTMB(
  density ~ (1 | site_individual),
  ziformula = ~ 1,
  family = nbinom2,  
  data = wave4
)

 summary(model01)
 r.squaredGLMM(model01)
 AIC(model01)
```

```{r}
model02 <- glmmTMB(
  density ~ water_depth + slope_use + exposure + (1 | site_individual),
  ziformula = ~ water_depth + slope_use + exposure,
  family = nbinom2,
  data = wave4
)

summary(model02)
anova(model02, model01)
```

```{r}
model03 <- glmmTMB(
  density ~ water_depth + slope_use + (1 | site_individual),
  ziformula = ~ exposure,
  family = nbinom2,
  data = wave4
)

summary(model03)
anova(model03, model02)
```

```{r}
model04 <- glmmTMB(
  density ~ water_depth + slope_use,
  ziformula = ~ exposure,
  family = nbinom2,
  data = wave4
)

summary(model04)
anova(model04, model03)
```

#### VII. Final Models

##### VII.a. Global ZIM

```{r}
coords <- cbind(wave4$latitude, wave4$longitude)
distances <- seq(0.00012, 0.001, by = 0.000001)
values <- numeric(length(distances))
values[] <- NA  
mmi <- -Inf  
od <- NA
neighbors <- NULL
weights <- NULL
options(warn = -1)
for (i in seq_along(distances)) {
  neighbors <- dnearneigh(coords, d1 = 0, d2 = distances[i]) #insert cords
  if (sum(card(neighbors)) == 0) { next }
  dists <- nbdists(neighbors, coords) # and here
  weights <- nb2listw(neighbors, 
                        glist = lapply(dists, function(x) 1 / x), 
                        style = "W", 
                        zero.policy = TRUE)
  moran_results <- moran(wave4$density, weights, length(neighbors), Szero(weights)) #insert density here
  values[i] <- moran_results$I
  
  if (!is.na(values[i]) && values[i] > mmi) {
    mmi <- values[i]
    od <- distances[i]
    neighbors <- neighbors
    weights <- weights
  }
}
cat("Maximum Moran's I:", mmi, "\n")
cat("Optimal Distance:", od, "\n")

plot(neighbors,coords, col = "red")

# Compute Moran’s I
moran_test <- moran.test(wave4$density, listw = weights, zero.policy = TRUE)
print(moran_test)

# Global Moran's I is highly significant.
```

```{r}
wave4$spatial_lag1 <- NA
wave4$spatial_lag2 <- NA
wave4$spatial_lag3 <- NA

weights_list <- list(
  "A" = weights_a,
  "D" = weights_b,
  "F" = weights_c,
  "H" = weights_d
)

for (s in unique(wave4$site_individual)) {
  if (s %in% names(weights_list)) {
    site_subset <- wave4[wave4$site_individual == s, ]
    W <- weights_list[[s]]
    wave4$spatial_lag1[wave4$site_individual == s] <- lag.listw(W, site_subset$density)
    wave4$spatial_lag2[wave4$site_individual == s] <- lag.listw(W, wave4$spatial_lag1[wave4$site_individual == s])
    wave4$spatial_lag3[wave4$site_individual == s] <- lag.listw(W, wave4$spatial_lag2[wave4$site_individual == s])
  } else {
    wave4$spatial_lag1[wave4$site_individual == s] <- 0
    wave4$spatial_lag2[wave4$site_individual == s] <- 0
    wave4$spatial_lag3[wave4$site_individual == s] <- 0
  }
}
```

```{r}
wave4$spatial_lag1_g <- lag.listw(weights, wave4$density)

spatial_G_1 <- glmmTMB(density ~ spatial_lag1_g + 
                                (1 | site_individual),
                                ziformula = ~ 1,
                                family = nbinom1,
                                data = wave4)
 
summary(spatial_G_1)
 
spatial_G_2 <- glmmTMB(density ~ water_depth + 
                                slope_use + 
                                spatial_lag1_g + 
                                (1 | site_individual),
                                ziformula = ~ exposure,
                                family = nbinom2,
                                data = wave4)

summary(spatial_G_2)

anova(spatial_G_2, spatial_G_1)

spatial_G_3 <- glmmTMB(density ~ water_depth + 
                                slope_use + 
                                spatial_lag1_g,
                                ziformula = ~ exposure,
                                family = nbinom2,
                                data = wave4)

summary(spatial_G_3)

anova(spatial_G_3, spatial_G_2)

wave4$residuals <- residuals(spatial_G_3)
for (s in c("A", "D", "F", "H")) {
   site_subset <- wave4[wave4$site_individual == s, ]
   moran_test <- moran.test(site_subset$residuals, weights_list[[s]])
   print(paste("Moran's I for site", s, ":", moran_test$estimate["Moran I statistic"]))
 }
```

```{r}
wave4$predicted_count <- predict(spatial_G_3, type = "response")

ggplot(wave4, aes(x = water_depth, y = density)) +
  geom_jitter(aes(color = exposure), width = 0.1, height = 0.1, alpha = 0.4) +
  geom_smooth(aes(y = predicted_count, color = exposure), method = "loess", se = FALSE, size = 1.2) +
  labs(
    x = "Water Depth",
    y = "Observed Count",
    title = "Observed vs. Predicted Counts by Water Depth and Exposure",
    color = "Exposure"
  ) +
  theme_minimal()
```

##### VII.b. Site Specific Correlated - ZIM

```{r}
spatial1 <- glmmTMB(density ~ spatial_lag1 + 
                              (1 | site_individual),
                              ziformula = ~ 1,
                              family = nbinom2,
                              data = wave4)

summary(spatial1)

spatial2 <- glmmTMB(density ~ water_depth +
                              slope_use +
                              spatial_lag1 + 
                              (1 | site_individual),
                              ziformula = ~ exposure,
                              family = nbinom2,
                              data = wave4)
 
summary(spatial2)
 
anova(spatial2, spatial1)

spatial3 <- glmmTMB(density ~ water_depth +
                              slope_use +
                              spatial_lag1,
                              ziformula = ~ exposure,
                              family = nbinom2,
                              data = wave4)

summary(spatial3)

anova(spatial3, spatial2)

wave4$residuals_local <- residuals(spatial3)
 
for (s in c("A", "D", "F", "H")) {
   site_subset <- wave4[wave4$site_individual == s, ]
   moran_test <- moran.test(site_subset$residuals_local, weights_list[[s]])
   print(paste("Moran's I for site", s, ":", moran_test$estimate["Moran I statistic"]))
 }
 
wave4$predicted_count <- predict(spatial3, type = "response")

ggplot(wave4, aes(x = water_depth, y = density)) +
  geom_jitter(aes(color = exposure), width = 0.1, height = 0.1, alpha = 0.4) +
  geom_smooth(aes(y = predicted_count, color = exposure), method = "loess", se = FALSE, size = 1.2) +
  labs(
    x = "Water Depth",
    y = "Observed Count",
    title = "Observed vs. Predicted Counts by Water Depth and Exposure",
    color = "Exposure"
  ) +
  theme_minimal()

```

##### VII.c. Site Specific Correlated - Logistic Regression

```{r}
 wave4$presence_absence <- (wave4$presence_absence_sch + wave4$presence_absence_typ)

spatial_l1 <- glmmTMB(presence_absence ~ spatial_lag1 + 
                                        spatial_lag2 + 
                                        spatial_lag3 + 
                                        (1 | site_individual), 
                                        data = wave4, 
                                        family = binomial(link = "logit")) 

summary(spatial_l1)

spatial_l2 <- glmmTMB(presence_absence ~ exposure + 
                                        spatial_lag1 + 
                                        spatial_lag2 + 
                                        spatial_lag3 + 
                                        (1 | site_individual), 
                                        data = wave4, 
                                        family = binomial(link = "logit"))
 
summary(spatial_l2)

anova(spatial_l2, spatial_l1)

spatial_l3 <- glmmTMB(presence_absence ~ exposure + 
                                        spatial_lag1 + 
                                        spatial_lag2 + 
                                        spatial_lag3, 
                                        data = wave4, 
                                        family = binomial(link = "logit"))
 
summary(spatial_l3)

anova(spatial_l3, spatial_l2)
 
wave4$residuals_logistic <- residuals(spatial_l2)
 
for (s in c("A", "D", "F", "H")) {
   site_subset <- wave4[wave4$site_individual == s, ]
   moran_test <- moran.test(site_subset$residuals_logistic, weights_list[[s]])
   print(paste("Moran's I for site", s, ":", moran_test$estimate["Moran I statistic"]))
}

wave4$predicted_prob <- predict(spatial_l2, type = "response")

ggplot(wave4, aes(x = predicted_prob, y = presence_absence, color = exposure)) +
  geom_jitter(height = 0.05, width = 0, alpha = 0.3) +
  geom_smooth(method = "loess", formula = y ~ x, se = FALSE, size = 1.2) +
  labs(
    x = "Predicted Probability",
    y = "Observed Presence (1) / Absence (0)",
    title = "Observed vs. Predicted Probabilities by Exposure",
    color = "Exposure"
  ) +
  theme_minimal()
```
